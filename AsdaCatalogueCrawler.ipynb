{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce9c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import ElementNotInteractableException, TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e60604a",
   "metadata": {},
   "source": [
    "### Web Crawler\n",
    "This section will help in getting product URLs from a given product catalogue URL on ASDA groceries website.\n",
    "These catelogue URLs can be found on the sitemap - https://groceries.asda.com/sitemap-category.xml \n",
    "\n",
    "Note: To crawl or scrape data from a website, ensure that the URLs are allowed on robot.txt of the website.\n",
    "For ASDA robot.txt can be found here - https://groceries.asda.com/robots.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8b89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_product_urls(url: str):\n",
    "    \"\"\"\n",
    "    This function is used to get all the main product URLs from a given ASDA catalogue URL \n",
    "    (Sitemap: https://groceries.asda.com/sitemap-category.xml)\n",
    "    Input: url (String)\n",
    "    Output: product_url (DataFrame) with page number, product name, url\n",
    "    \"\"\"\n",
    "    print(f\"Input URL: {url}\")\n",
    "    # Initialize Firefox WebDriver\n",
    "    driver = webdriver.Firefox()\n",
    "    \n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Alert\n",
    "    wait = WebDriverWait(driver, 10)  \n",
    "    wait.until(EC.presence_of_element_located((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "    alert = driver.find_element_by_id('onetrust-accept-btn-handler')\n",
    "    alert.click()\n",
    "    \n",
    "    # Check if it is the right page\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 30)  # Adjust the timeout as needed\n",
    "        main_content = wait.until(EC.presence_of_element_located((By.XPATH,'//div[@data-module-type=\"ProductListing\"]')))\n",
    "    except TimeoutException as e:\n",
    "        print(\"Could not load this page to crawl for product URLs, please check the URL provided.\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # Get number of pages\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)  \n",
    "        page = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\".co-pagination__last-page\")))\n",
    "        max_no_pages = int(page.text)  \n",
    "    except TimeoutException as e:\n",
    "        max_no_pages = 1\n",
    "    \n",
    "    print(f\"Max number of pages = {max_no_pages}\")\n",
    "    product_urls_df = pd.DataFrame(columns=[\"page_no\", \"product_name\", \"url\"])\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for i in range(max_no_pages):\n",
    "        # Get main product listing\n",
    "        wait = WebDriverWait(driver, 30)  # Adjust the timeout as needed\n",
    "        main_content = wait.until(EC.presence_of_element_located((By.XPATH,'//div[@data-module-type=\"ProductListing\"]')))\n",
    "        \n",
    "        # Get all URLs (anchor tags)\n",
    "        elements = main_content.find_elements_by_class_name(\"co-product__anchor\")\n",
    "        for ele in elements:\n",
    "            # Save page number, product name and link\n",
    "            product_details = {\"page_no\": i+1, \"product_name\": ele.text, \"url\": ele.get_attribute(\"href\")}\n",
    "            product_urls_df = pd.concat([product_urls_df, pd.DataFrame(product_details, index=[0])], ignore_index=True)\n",
    "        \n",
    "        # For the last page, do not click on right arrow to go to next page\n",
    "        if i==max_no_pages-1:\n",
    "            continue\n",
    "        \n",
    "        # Click on next page arrow\n",
    "        wait = WebDriverWait(driver, 10)  # Adjust the timeout as needed\n",
    "        retry = 0\n",
    "        while retry<3:\n",
    "            try:\n",
    "                right_arrow = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR,\".co-pagination__arrow--right\")))\n",
    "                right_arrow.click()\n",
    "                break\n",
    "            except ElementNotInteractableException as e:\n",
    "                retry+=1\n",
    "                time.sleep(5)\n",
    "    driver.quit()\n",
    "    product_urls_df[\"parent_url\"] = url\n",
    "    print(f\"Number of URLs extracted: {product_urls_df.shape[0]}\")\n",
    "    return product_urls_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f813d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of pages = 4\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    url = \"https://groceries.asda.com/aisle/food-cupboard/cereals-cereal-bars/everyday-family-cereals/1215337189632-1215337194729-1215650880276\"\n",
    "    product_urls = get_all_product_urls(url)\n",
    "    print(f\"Number of URLs extracted from each page: \")\n",
    "    print(product_urls.groupby(by=\"page_no\")[\"url\"].count().reset_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
