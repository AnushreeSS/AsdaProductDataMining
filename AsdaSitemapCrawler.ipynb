{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5aea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.robotparser as urobot\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec928d",
   "metadata": {},
   "source": [
    "## Scanning Robot.txt using Robot Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd6b57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRobotParser(robo_url):\n",
    "    rp = urobot.RobotFileParser()\n",
    "    rp.set_url(robo_url)\n",
    "    rp.read()\n",
    "    return rp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacfa40d",
   "metadata": {},
   "source": [
    "## Scanning Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325c9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_sitemap(sitemap_url:str, robot_parser:urobot.RobotFileParser = None):\n",
    "    resp = requests.get(sitemap_url)\n",
    "    sitemap = BeautifulSoup(resp.content, 'xml')\n",
    "    urls = [url.text for url in sitemap.find_all('loc')]\n",
    "    print(f\"Number of URLs: {len(urls)}\")\n",
    "    if robot_parser:\n",
    "        # Filtering URLs using Robot Parser\n",
    "        urls = list(filter(lambda url: robot_parser.can_fetch(\"*\", url), urls))\n",
    "        print(f\"Number of URLs after parsing through robot parser: {len(urls)}\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d14633",
   "metadata": {},
   "source": [
    "## Categorising Sitemap URLs\n",
    "In this case, the category sitemap is scanned to get all URLs and provide categories it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21a0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorised_urls_from_sitemap(sitemap_url:str, robo_parser=None):  \n",
    "    urls = get_urls_from_sitemap(sitemap_url, robo_parser)\n",
    "    # Categorising URLs based on parts of the URL path\n",
    "    urls_df = pd.DataFrame(columns=[\"Type\", \"Category\", \"SubCategory\", \"URL\"])\n",
    "    for url in urls:\n",
    "        url_parts = url.split(\"/\")[3:] # First 3 will include HTTP protocol and host name\n",
    "        temp_dict = {\"Type\":url_parts[0] if len(url_parts)>0 else np.nan,\n",
    "                     \"Category\":url_parts[1] if len(url_parts)>1 else np.nan,\n",
    "                     \"SubCategory\":url_parts[2] if len(url_parts)>2 and not url_parts[2].isdigit() else np.nan,\n",
    "                     \"URL\":url}\n",
    "        urls_df = pd.concat([urls_df, pd.DataFrame(temp_dict, index=[0])], ignore_index=True)\n",
    "    return urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73482a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Parsing Robot.txt ***\n",
      "Host: groceries.asda.com | Sitemaps: ['https://groceries.asda.com/sitemap-index.xml']\n",
      "\n",
      "*** Scanning Main Sitemap ***\n",
      "Number of URLs: 6\n",
      "Number of sitemaps found: 6\n",
      "['https://groceries.asda.com/sitemap-category.xml', 'https://groceries.asda.com/sitemap-event-pages.xml', 'https://groceries.asda.com/sitemap-products.xml', 'https://groceries.asda.com/sitemap-recipes-categories.xml', 'https://groceries.asda.com/sitemap-recipes.xml', 'https://groceries.asda.com/sitemap-special-offers.xml']\n",
      "\n",
      "*** Scanning Category Sitemap ***\n",
      "Number of URLs: 3705\n",
      "Number of URLs after parsing through robot parser: 3705\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print(\"*** Parsing Robot.txt ***\")\n",
    "    robo_url = \"https://groceries.asda.com/robots.txt\"\n",
    "    robo_parser = getRobotParser(robo_url)\n",
    "    print(f\"Host: {robo_parser.host} | Sitemaps: {robo_parser.sitemaps}\")\n",
    "    \n",
    "    # Get URLs from index sitemap\n",
    "    print(\"\\n*** Scanning Main Sitemap ***\")\n",
    "    sitemaps = get_urls_from_sitemap(robo_parser.sitemaps[0])\n",
    "    print(f\"Number of sitemaps found: {len(sitemaps)}\")\n",
    "    print(sitemaps)\n",
    "    \n",
    "    # Get URLs from category sitemap\n",
    "    print(\"\\n*** Scanning Category Sitemap ***\")\n",
    "    sitemap_url= 'https://groceries.asda.com/sitemap-category.xml'\n",
    "    urls_df = get_categorised_urls_from_sitemap(sitemap_url, robo_parser)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
